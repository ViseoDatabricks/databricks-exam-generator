{
  "0": {
    "questions": "A data engineer has a Python workload they want to run as a job. The code for the workload is located in an external cloud storage location.\n\nWhich of the following task types and sources can the data engineer use to configure this job? Select one response.\n",
    "choices": [
      "Notebook with local path source",
      "Python script with Workspace source",
      "Python script with DBFS source",
      "Delta Live Tables pipeline with Workspace source",
      "Notebook with DBFS source"
    ],
    "expected_answers": 1,
    "answers": 3
  },
  "1": {
    "questions": "A data engineer needs to view whether each task within a job run succeeded.\n\nWhich of the following steps can the data engineer complete to view this information? Select one response.\n",
    "choices": [
      "They can review the task output from the notebook commands.",
      "They can review job run output from the resultant directed acyclic graph (DAG).",
      "They can review the job run history from the Job run details page.",
      "They can review the job run history from the Workflow Details page.",
      "They can review the task history by clicking on each task in the workflow."
    ],
    "expected_answers": 1,
    "answers": 3
  },
  "2": {
    "questions": "A data engineer is running a workflow orchestration on a shared job cluster. They notice that the job they are running is failing and want to use the repair tool to fix the pipeline.\n\nWhich of the following statements describes how Databricks assigns a cluster to the repaired job run? Select one response.\n",
    "choices": [
      "The platform administrator can set who can view job results or manage runs of a job with access control lists at the user or group level.",
      "The workspace administrator can set the maximum number of users who can access the table at the group level.",
      "The platform administrator can set who can grant access permissions or view job history with access control lists at the user level.",
      "The platform administrator can set who can search for jobs by id or grant access permissions with access control lists at the user or group level.",
      "The workspace administrator can set the maximum number of users who can access the table at the user level."
    ],
    "expected_answers": 1,
    "answers": 1
  },
  "3": {
    "questions": "A data engineer needs to view the metadata concerning the order that events in a DLT pipeline were executed.\nWhich of the following steps can the data engineer complete to view this information? Select one response.\n",
    "choices": [
      "The data engineer can view the DLT metrics from the bar graph that is generated within the notebook.",
      "The data engineer can query the metrics column of the event log for DLT metrics",
      "The data engineer can view the DLT metrics from the Pipeline Details page.",
      "The data engineer can query the event log from a new notebook.",
      "The data engineer can view the DLT metrics from the resultant Directed Acyclic Graph (DAG)."
    ],
    "expected_answers": 1,
    "answers": 3
  },
  "4": {
    "questions": "A data engineer is using Workflows to run a multi-hop (medallion) ETL workload. They notice that the workflow will not complete because one of the tasks is failing.\n\nWhich of the following describes the order of execution when running the repair tool? Select one response.\n",
    "choices": [
      "The data engineer can use the repair feature to re-run only the failed sub-tasks.",
      "The data engineer can use the repair feature to re-run only the failed task and the task it depends on.",
      "The data engineer can use the repair feature to re-run only the sub-tasks of the failed task.",
      "The data engineer can use the repair feature to re-run only the failed task and the tasks following it.",
      "The data engineer can use the repair feature to re-run only the failed task and sub-tasks."
    ],
    "expected_answers": 1,
    "answers": 5
  },
  "5": {
    "questions": "A data engineer is running a job every 15 minutes. They want to stop the job schedule for an hour before starting it again.\nWhich of the following allows the data engineer to stop the job during this interval and then start it again without losing the job’s configuration? Select two responses.\n",
    "choices": [
      "They can stop the job schedule and then refresh the query within the job after an hour.",
      "They can click Pause in the Job details panel.",
      "They can set the Schedule Type to Manual in the Job details panel and change it back to Scheduled after an hour.",
      "They can detach the job from its accompanying dashboard and then reattach and refresh the dashboard after an hour",
      "They can stop the job schedule and then refresh the notebook that is attached to the task after an hour."
    ],
    "expected_answers": 2,
    "answers": [2,3]
  },
  "6": {
    "questions": "Which of the following statements about the advantages of using Workflows for task orchestration are correct? Select three responses.",
    "choices": [
      "Workflows is fully managed, which means users do not need to worry about infrastructure.",
      "Workflows supports built-in data quality constraints for logging purposes.",
      "Workflows provides a centralized repository for data visualization tools.",
      "Workflows can be configured to use external access control permissions.",
      "Workflows can be used to make external API calls."
    ],
    "expected_answers": 3,
    "answers": [1,2,5]
  },
  "7": {
    "questions": "Which of the following configurations are required to specify when scheduling a job? Select two responses.\n",
    "choices": [
      "Trigger type",
      "Start time",
      "Time zone",
      "Trigger frequency",
      "Maximum number of runs"
    ],
    "expected_answers": 2,
    "answers": [1,4]
  },
  "8": {
    "questions": "A data engineer needs to configure the order of tasks to run in their ETL workload. The workload has two tasks, Task A and Task B, where Task B can only be run if Task A succeeds.\n\nWhich of the following statements describes the dependencies that the data engineer needs to configure and the order they need to be run in? Select one response.\n",
    "choices": [
      "They need to add Task B as a dependency of Task A and run the tasks in parallel.",
      "They need to add Task B as a subtask of Task A and run the tasks in parallel.",
      "They need to add Task B as a dependency of Task A and run the tasks in sequence.",
      "They need to add Task B as a subtask of Task A and run the tasks in sequence.",
      "They need to add Task A as a dependency of Task B and run the tasks in sequence."
    ],
    "expected_answers": 1,
    "answers": 3
  },
  "9": {
    "questions": "A data engineer has multiple data sources that they need to combine into one. The combined data sources then need to go through a multi-task ETL process to refine the data using multi-hop (medallion) architecture. It is a requirement that the source data jobs need to be run in parallel.\n\nWhich of the following workflow orchestration patterns do they need to use to meet the above requirements? Select one response.\n",
    "choices": [
      "Fan-out to sequence pattern",
      "Funnel to sequence pattern",
      "Parallel to multi-sequence pattern",
      "Sequence to fan-out pattern",
      "Funnel to fan-out pattern"
    ],
    "expected_answers": 1,
    "answers": 2
  },
  "10": {
    "questions": "A data engineer has a notebook in a remote Git repository. The data from the notebook needs to be ingested into a second notebook that is hosted in Databricks Repos.\n\nWhich of the following approaches can the data engineer use to meet the above requirements? Select one response.\n",
    "choices": [
      "The data engineer can configure the notebook in the remote repository as a job and make the second notebook dependent on it.",
      "The data engineer can configure the notebook in a new local repository as a job and make the second notebook dependent on it.",
      "The data engineer can configure the notebook in a new local repository as a task and make the second notebook dependent on it.",
      "The data engineer can configure the notebook in the remote repository as a task and make it a dependency of the second notebook.",
      "The data engineer can configure the notebook in a new local remote repository as a job and make it a dependency of the second notebook."
    ],
    "expected_answers": 1,
    "answers": 1
  },
  "11": {
    "questions": "Which of the following workloads can be configured using Databricks Workflows? Select three responses.",
    "choices": [
      "A data analysis job that uses R notebooks",
      "A job with Python, SQL, and Scala tasks",
      "A job running on a triggered schedule with dependent tasks",
      "A custom task where data is extracted from a JAR file",
      "An ETL job with batch and streaming data"
    ],
    "expected_answers": 3,
    "answers": [3,4,5]
  },
  "12": {
    "questions": "Which of the following components are necessary to create a Databricks Workflow? Select three responses.",
    "choices": [
      "Tasks",
      "Cluster",
      "Experiment",
      "Schedule",
      "Alert"
    ],
    "expected_answers": 3,
    "answers": [1,2,4]
  },
  "13": {
    "questions": "A data engineer has a workload that includes transformations of batch and streaming data, with built-in constraints to ensure each record meets certain conditions.\n\nWhich of the following task types is considered best practice for the data engineer to use to configure this workload? Select one response.\n",
    "choices": [
      "Python script",
      "dbt",
      "Notebook",
      "Spark submit",
      "Delta Live Tables pipeline"
    ],
    "expected_answers": 1,
    "answers": 5
  },
  "14": {
    "questions": "A data engineering team needs to be granted access to metrics on a job run. Each team member has user access without any additional privileges.\n\nWhich of the following tasks can be performed by an administrator so that each member of the team has access to the metrics? Select one response.\n",
    "choices": [
      "The platform administrator can set who can grant access permissions or view job history with access control lists at the user level.",
      "The platform administrator can set who can search for jobs by id or grant access permissions with access control lists at the user or group level.",
      "The workspace administrator can set the maximum number of users who can access the table at the user level.",
      "The platform administrator can set who can view job results or manage runs of a job with access control lists at the user or group level.",
      "The workspace administrator can set the maximum number of users who can access the table at the group level."
    ],
    "expected_answers": 1,
    "answers": 4
  },
  "15": {
    "questions": "Which of the following are managed by Databricks Workflows? Select three responses.",
    "choices": [
      "Error reporting",
      "Git version control",
      "Task orchestration",
      "Cluster management",
      "Table access control lists (ACLs)"
    ],
    "expected_answers": 3,
    "answers": [1,3,4]
  },
  "16": {
    "questions": "Which of the following tools can be used to create a Databricks Job? Select three responses.",
    "choices": [
      "Jobs REST API",
      "External Git repository",
      "Data Explorer",
      "Databricks CLI",
      "Job Scheduler UI"
    ],
    "expected_answers": 3,
    "answers": [1,4,5]
  },
  "17": {
    "questions": "A data engineer needs their pipeline to run every 12 minutes.\n\nWhich of the following approaches automates this process? Select one response.\n",
    "choices": [
      "This type of scheduling cannot be done with Databricks Workflows.",
      "The data engineer can set the job’s schedule with custom cron syntax.",
      "The data engineer can call the Apache AirFlow API to set the job’s schedule.",
      "The data engineer can use custom Python code to set the job’s schedule.",
      "The data engineer can manually pause and start the job every 12 minutes."
    ],
    "expected_answers": 1,
    "answers": 2
  },
  "18": {
    "questions": "A data engineer has run a Delta Live Tables pipeline and wants to see if there are records that were not added to the target dataset due to constraint violations.\n\nWhich of the following approaches can the data engineer use to view metrics on failed records for the pipeline? Select two responses.\n",
    "choices": [
      "They can view the percentage of records that failed each data expectation from the Pipeline details page.",
      "They can view the duration of each task from the Pipeline details page.",
      "They can view how many records were added to the target dataset from the accompanying SQL dashboard.",
      "They can view how many records were dropped from the Pipeline details page.",
      "They can view information on the percentage of records that succeeded each data expectation from the audit log."
    ],
    "expected_answers": 2,
    "answers": [1,2]
  },
  "19": {
    "questions": "A data engineer is running multiple notebooks that are triggered on different job schedules. Each notebook is part of a different task orchestration workflow. They want to use a cluster with the same configuration for each notebook.\n\nWhich of the following describes how the data engineer can use a feature of Workflows to meet the above requirements? Select one response.\n",
    "choices": [
      "They can use an alert schedule to swap out the clusters after each job has completed.",
      "They can configure each notebook’s job schedule to swap out the cluster after the job has finished running.",
      "They can refresh the cluster after each notebook has finished running in order to use the cluster on a new notebook.",
      "They can use a sequence pattern to make the notebooks depend on each other in a task orchestration workflow and run the new workflow on the cluster.",
      "They can use the same cluster to run the notebooks as long as the cluster is a shared cluster."
    ],
    "expected_answers": 1,
    "answers": 5
  },
  "20": {
    "questions": "A data engineering team has been using a Databricks SQL query to monitor the performance of an ELT job. The ELT job is triggered when a specific number of input records are ready to be processed. The Databricks SQL query returns the number of records added since the job’s most recent runtime. The team has manually reviewed some of the records and knows that at least one of them will be successfully processed without violating any constraints.\n\nWhich of the following approaches can the data engineering team use to be notified if the ELT job did not complete successfully? Select one response.\n",
    "choices": [
      "They can set up an alert for the job to notify them if the returned value of the SQL query is less than 1.",
      "They can set up an alert for the job to notify them when a record has been flagged as invalid.",
      "They can set up an alert for the job to notify them when a record has been added to the target dataset.",
      "They can set up an alert for the job to notify them when a constraint has been violated.",
      "This type of alerting is not possible in Databricks."
    ],
    "expected_answers": 1,
    "answers": 1
  },
  "21": {
    "questions": "A data engineer has a job that creates and displays a result set of baby names by year, where each row has a unique year. They want to display the results for baby names from the past three years only.\n\nWhich of the following approaches allows them to filter rows from the table by year? Select one response.\n",
    "choices": [
      "They can re-run the job with new parameters for the task.",
      "They can edit the table to remove certain rows in the Job Details page.",
      "They can add widgets to the notebook and re-run the job.",
      "They can add a filter condition to the job’s configuration.",
      "They can add an import statement to input the year."
    ],
    "expected_answers": 1,
    "answers": 1
  },
  "22": {
    "questions": "A lead data engineer needs the rest of their team to know when an update has been made to the status of a job run within a Databricks Job.\n\nHow can the data engineer notify their team of the status of the job? Select one response.\n",
    "choices": [
      "They can schedule an email alert to the whole team for when the job completes.",
      "They can schedule a Dashboard alert to the whole team for when the job succeeds.",
      "They can schedule a Dashboard alert to themselves for when the job succeeds.",
      "They can schedule an email alert to themselves for when the job begins.",
      "They can schedule a Dashboard alert to a group containing all members of the team for when the job completes."
    ],
    "expected_answers": 1,
    "answers": 1
  },
  "23": {
    "questions": "A data engineer has a notebook that ingests data from a single data source and stores it in an object store. The engineer has three other notebooks that read from the data in the object store and perform various data transformations. The engineer would like to run these three notebooks in parallel after the ingestion notebook finishes running.\n\nWhich of the following workflow orchestration patterns do they need to use to meet the above requirements? Select one response.\n",
    "choices": [
      "Sequence pattern",
      "Fan-out pattern",
      "Funnel pattern",
      "Multi-sequence pattern",
      "Hourglass pattern"
    ],
    "expected_answers": 1,
    "answers": 2
  },
  "24": {
    "questions": "Which of the following task types can be combined into a single workflow? Select three responses.\n",
    "choices": [
      "JAR files",
      "Python notebooks",
      "Alert destinations",
      "SQL warehouses",
      "SQL notebooks"
    ],
    "expected_answers": 3,
    "answers": [1,2,5]
  }
}